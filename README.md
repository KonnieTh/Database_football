# football database

Για συλλογή δεδομένων μέσω web-scraping:
(1) Σε περίπτωση που δεν είναι εγκατεστημένες οι βιβλιοθήκες pandas, beautifulsoup4, requests και xlsxwriter,lxml 
πρέπει να γίνει η εγκατάστασή μέσω του terminal(γραμμή εντολών) του υπολογιστή. Αυτό γίνεται μέσω της εντολής 
pip install <όνομα της βιβλιοθήκης>. Περισσότερες πληροφορίες για αυτές παρέχονται στο documentation της python.

(2) Για να τρέξουμε τα αρχεία του web-scraping για τη συλλογή των δεδομένων των ιστοσελίδων:
Αρχικά, τρέχουμε πρώτα τα αρχεία κώδικα με ονόματα matches, stadiums, players, belongs, coaches, Team. 
Από αυτά τα αρχεία προκύπτουν οι πίνακες των αγώνων(Match_), των σταδίων(Stadium), των παικτών(Player),
των συμβολαίων των παικτών(belongs), των προπονητών(Coach) και των ομάδων(Team) αντίστοιχα. 
Από το αρχείο matches.py προκύπτει επίσης και ένα επιπλέον αρχείο excel με όνομα matches2, 
το οποίο χρησιμοποιείται σε επόμενα αρχεία κώδικα και το οποίο περιέχει επιπλέον τις ομάδες που πήραν 
μέρος σε κάθε αγώνα καθώς και το λινκ του κάθε αγώνα. Τα αρχεία κώδικα που χρειάζονται αυτό το αρχείο excel 
είναι τα enters.py, attributes_violation.py, substitutes.py,referee_inspects.py και το participates.py, 
τα οποία πρέπει να βρίσκονται όλα στον ίδιο φάκελο. Τρέχοντάς τα, προκύπτουν excel αρχεία με δεδομένα 
για τους πίνακες enters, attributes_violation, substitutes, referee, inspects και participates. 

Λόγω του ότι ορισμένα δεδομένα από την ιστοσελίδα δεν ήταν σωστά, κάνουμε ορισμένες αλλαγές:
-Διαγράφουμε από το αρχεία excel Players και participates τους παίκτες με player_ID 172, 331, 370, 409, 458, 
 463, 495, 538, 559, 596, 601. Αυτούς του διαγράφουμε διότι είναι παίκτες που εμφανίζονται σε δύο ομάδες και άρα έχουν δύο IDs.

-Από το participates excel ακόμη, διαγράφουμε στον παίκτη με player_ID 148 διαγράφουμε τα στατιστικά του από τον αγώνα με match_ID 363,
 διότι είχε κενά στατιστικά και δημιουργούσε πρόβλημα στη βάση μας.
Ακόμη, επειδή θεωρήσαμε ότι σε κάθε αγώνα δεν έχουμε επιπλέον καθυστερήσεις (αφού δεν υπήρχε σαν δεδομένο στην ιστοσελίδα), 
φτιάχνουμε τους χρόνους συμμετοχής των παικτών στους αγώνες, τους χρόνους που γίνονται οι αλλαγές και τους χρόνους που δίνονται κάρτες, 
ώστε σε κάθε αγώνα και για κάθε ομάδα που συμμετέχει σε αυτόν, οι παίκτες της να έχουν χρόνο συμμετοχής το πολύ 990 λεπτά, 
τρέχουμε τo αρχείο fix_times_new. Αυτό το αρχείο κάνει αυτή την τροποποίηση και δημιουργεί τα τελικά αρχεία δεδομένων για τους πίνακες participates, 
substitutes και attributes_violation. Το συγκεκριμένο αρχείο κώδικα πρέπει να βρίσκεται επίσης μαζί με τα υπόλοιπα αρχεία excel που φτιάξαμε.

Επιπλέον, διαγράφουμε από το τελικό αρχείο excel του participates τους εξής παίκτες:
-player_ID = 355 στο match_ID =130 και εκεί που έχει χρόνο συμμετοχής 64 λεπτά
-player_ID = 33 στο match_ID =208 και εκεί που έχει χρόνο συμμετοχής 87 λεπτά
-player_ID = 383 στο match_ID =206 και εκεί που έχει χρόνο συμμετοχής 47 λεπτά
Αυτούς επίσης τους βγάζουμε και από το attributes_violation. Τους συγκεκριμένους παίκτες τους βγάζουμε διότι η ιστοσελίδα από όπου πήραμε τα δεδομένα μας, 
θεωρούσε τις εκτελέσεις πέναλτι, τις αποκρούσεις πέναλτι, τις ασσίστς και τα αυτογκόλ ως καινούργια συμμετοχή ή ως κάρτες.

Τέλος προσθέτουμε στο αρχείο excel participates, έναν παίκτη με τα εξής χαρακτηριστικά:
player_ID = 368, match_ID= 217 , start = Y , goals = 0, participation_in_match=79, saves = NULL, position = MF , 
start_of_participation =0 και end_of_participation = 79, ενώ τα άλλα στοιχεία είναι 0.

Ο λόγος που το κάνουμε αυτό είναι γιατί ο συγκεκριμένος βγήκε από τον αγώνα ως τραυματίας και δεν αντικαταστάθηκε από κάποιον άλλο παίκτη.

Έχοντας, τρέξει όλα αυτά τα αρχεία προκύπτουν τα αρχεία excel με τα δεδομένα της βάσης μας τα οποία και πρέπει να αποθηκευτούν σε φάκελο με όνομα excel_csv 
(Αυτά τα αρχεία excel υπάρχουν έτοιμα και στο φάκελο με όνομα excel_csv).

(3) Δημιουργία Βάσης: Για τη δημιουργία της βάσης μας, εκτελούμε τον κώδικα της python με όνομα initialize_database. 
Αυτό το αρχείο δημιουργεί τη βάση δεδομένων μας και ενσωματώνει τα δεδομένα των αρχείων excel και csv που πήραμε από το προηγούμενο βήμα σε αυτή,
ενώ ακόμη ενσωματώνει και τα δεδομένα υπολογιζόμενων γνωρισμάτων. Προκειμένου να είναι δυνατή η εκτέλεση του αρχείου αυτού, 
πρέπει να είναι εγκατεστημένες οι βιβλιοθήκες pandas,sqlite3 και time της python.
 Ο τρόπος εγκατάστασής τους είναι ο ίδιος με αυτό που είπαμε και στο προηγούμενο βήμα. 
Η βάση δεδομένων που προκύπτει υπάρχει στον παραδιδόμενο φάκελο με όνομα project το οποίο και μπορεί να ανοίξει κανείς μέσω της εφαρμογής 
της SQLite κάνοντας open database και επιλέγοντας το συγκεκριμένο αρχείο. Για την σωστή λειτουργία του προγράμματος αυτού, 
θα πρέπει ο χώρος στον οποίο βρίσκεται να περιλαμβάνει οπωσδήποτε τον φάκελο excel_csv. Τα αρχεία αυτά excel τα διαβάζουμε 
μέσω της εντολής pd.read_<τύπος_αρχείου>(r'excel_csv\< όνομα_αρχείου>.<τύπος αρχείου>').

(4) Εκτέλεση Εφαρμογής: Η εκτέλεση της εφαρμογής γίνεται εκτελώντας τον κώδικα της python με όνομα database_final. 
Συνιστούμε το τρέξιμο του κώδικα στο ίδιο χώρο με το αρχείο της βάσης και με τον φάκελο με όνομα images, 
διότι χρησιμοποιούμε εικόνες στην εφαρμογή μας που υπάρχουν σε αυτό το φάκελο. Προκειμένου να είναι δυνατή η εκτέλεση του αρχείου αυτού, 
πρέπει να είναι εγκατεστημένες οι βιβλιοθήκες tkinter, sqlite3 και time της python.

ΣΗΜΕΙΩΣΗ: Τα προγράμματα του Web Scraping τρέχουν αρκετά αργά λόγω του όγκου δεδομένων
